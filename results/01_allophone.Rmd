---
title: "Allophone"
output: github_document
---

01でallophoneの効果を見るが、そもそも受け入れられるかを
03のcatで見ている。音声としての自然さを見れる。
論文としてのまとめ方はkilpatrickを参考に進める。
https://drive.google.com/drive/folders/1-Dfuzcas58w9ERrDzoPggfp9vIAjPo7_

被験者ごとに voiced-voiceless を求めるか。

- 独立変数
  - 被験者要因(居住歴|無声化率)
- 従属変数
1. 被験者ごとのデータをまとめる
  * 産出課題: 無声化率(環境毎)
  * *居住歴* (*tokyo_kinki_ratio*, span_kinki, span_kinki_span_other, span_kinki_span_tokyo (閾値が面倒))
  - ランダム効果  
1. 被験者ごとのデータをまとめる
  * 無声化
    * AXB
    * 判定課題
  * トーンの復元
  
タスクの確認

deltaを分析したほうが良いかも

item_id がないのは練習問題

変なものと評価されているデータは除外する。

* production
* axb
  * item_id でタスクが分かれる
* cat
  * item_id でタスクが分かれる
* rate(音としてどれくらい良いか)
  * item_id でタスクが分かれる
  
無声化
https://docs.google.com/document/d/1OKkkZ4sELLtTBjiBWUG_P37EsWdFsidrGv0eOqiwIic/edit?usp=sharing

```{r setup, include=FALSE}
library(dplyr)
library(ggplot2)
library(readr)
library(stringr)
library(tidyr)
source("script/subject_info.R")
source("script/results_pitch.R") # load mutated data
```

```{r}
item_list <- read_csv("../src/list/axb_list.csv") %>%
  select(c(item_id, type, condition, correct, item_a, item_x, item_b, c1, c2))
subject_info_sone <- read_csv("csv/illusory-vowel-keihan.csv") %>%
  extract_columns(data_src = "sone")
subject_info_cw <- read_csv("csv/illusory-vowel-keihan-cw.csv") %>%
  extract_columns(data_src = "cw")
subject_info <- rbind(subject_info_sone, subject_info_cw)

results <- rbind(
  read_csv("csv/illusory-vowel-keihan.csv") %>% mutate(data_src = "sone"),
  read_csv("csv/illusory-vowel-keihan-cw.csv") %>% mutate(data_src = "cw")
) %>%
  filter(task == "axb") %>%
  left_join(subject_info, by = c("run_id", "data_src")) %>%
  select(c(run_id, item_id, is_correct, rt, span_kinki, span_tokyo, span_unknown, age, data_src, correct)) %>%
  mutate(
    item_id = as.numeric(item_id),
    rt = as.numeric(rt),
    item_id = as.numeric(item_id),
    is_correct = is_correct == "true"
  ) %>%
  drop_na() %>%
  mutate(
    tokyo_kinki_ratio = (span_tokyo - span_kinki) / age
  ) %>%
  filter(age > 18) %>%
  merge(item_list, on = "item_id") %>%
  filter(type == "target") %>% 
  mutate(
    phoneme_envs = paste0(c1, "-", c2),
    subject_id = paste0(data_src, "-", run_id),
    pair = case_when(
      phoneme_envs %in% c("s-p", "z-b") ~ "s-p",
      phoneme_envs %in% c("p-s", "b-z") ~ "p-s",
      phoneme_envs %in% c("k-t", "g-d") ~ "k-t",
      phoneme_envs %in% c("ts-k", "dz-g") ~ "ts-k"
    )
  )
nrow(results)
```

```{r}
# 右裾の確認, 10s 以上は外れ値とする
results %>% select(rt) %>% unlist %>% hist(breaks = 100)
# 左裾はそこまで以上な値はなさそう.
results %>% filter(rt<400) %>% select(rt) %>% unlist %>% hist(breaks = 100)
# 対数を取って反応の正規分布を確認, 2sd以上の反応は除外
results_filtered = results # %>% filter(between(rt,0,5000))
hist(log(results_filtered$rt))
# sd_log_rt = 2*sd(log(results_filtered$rt))
# mean_log_rt = mean(log(results_filtered$rt))
# floor_log_rt = mean_log_rt-sd_log_rt
# ceil_log_rt = mean_log_rt+sd_log_rt
# print(floor_log_rt)
# print(ceil_log_rt)
# results_filtered = results_filtered %>% mutate(log_rt = log(rt)) %>% filter(between(log_rt,floor_log_rt,ceil_log_rt))
results_filtered$rt %>% summary() %>% print
print(nrow(results_filtered))
```


```{r}
results_mean = results_filtered %>%
  group_by(subject_id, tokyo_kinki_ratio, pair, condition, phoneme_envs) %>%
  # item_id があると ebzo ebuzo ebuzo と ebuzo ebuzo ebzo が別になり
  # 平均ができない
  # group_by(item_id, subject_id, tokyo_kinki_ratio, pair, condition, phoneme_envs) %>%
  mutate(
    is_correct_mean = mean(is_correct),
    rt_mean = mean(rt),
  ) %>%
  select(c(subject_id, tokyo_kinki_ratio, span_tokyo, span_kinki, pair, condition, phoneme_envs, is_correct_mean, rt_mean)) %>% droplevels()%>% distinct()

results_mean
```

アイテムリストのマージ

概ね右肩さがり（東京のほうが錯覚率低い？）？
いや、子音のタイプで傾向が似ているのは面白い。
それもまた、一つの分析になる。

あと、東京保のほうが全体的に弁別精度が低い？

spで東京は低く、psでは若干たかい。
環境の効果がある？ ps では識別できて sp では識別できない。
音響情報の効果なら、方言差はでないはず。

パターンがktとかgtで似ているのは、共通した音響情報の効果か。
いや、なんにせよ居住歴の効果がでているのは興味深い。

そもそもktをベースとしたとき、全部が低い。とくにspの正答率が低い。
これは先行研究通り。

> pairp-s                                    -3.559e-02  1.497e-02  2.187e+03  -2.378  0.01749 *  
> pairs-p                                    -4.223e-02  1.497e-02  2.187e+03  -2.822  0.00482 ** 
> pairts-k                                   -2.876e-02  1.497e-02  2.187e+03  -1.922  0.05474 .  

統計上は、s-pのvoiced で正答率が上がる
（逆に言うと、spで下がる）のも先行研究通り。

> conditionvoiced                            -3.200e-02  1.497e-02  2.187e+03  -2.138  0.03261 *  
> conditionvoiced:pairp-s                     4.954e-02  2.117e-02  2.187e+03   2.341  0.01934 *  
> conditionvoiced:pairs-p                     6.345e-02  2.117e-02  2.187e+03   2.998  0.00275 ** 
> conditionvoiced:pairts-k                   -2.448e-03  2.117e-02  2.187e+03  -0.116  0.90793   

音素ごとに傾向が分かれるのが意味不明すぎる。
ただ、spにフォーカスする。そうすると、そこでは効果はない。

> pairp-s:tokyo_kinki_ratio                   4.874e-02  1.902e-02  2.187e+03   2.563  0.01043 *  
> pairs-p:tokyo_kinki_ratio                  -2.621e-03  1.902e-02  2.187e+03  -0.138  0.89040    
> pairts-k:tokyo_kinki_ratio                  7.642e-02  1.902e-02  2.187e+03   4.019 6.05e-05 ***

つまり、音響情報の効果ではないだろうか、という筋になる。
すごくクリーンな結果。ただ、これはspに限定した話で、
p-sやts-kでは統一ではない。

データが不当に多くなっていないか確認（理屈では、倍になったところで有意にはならないが）

いや、もとの話は spなどのような、後ろが停止だったもののはず。

```{r}
results_mean %>%
  ggplot() +
  facet_grid(pair ~ condition) +
  geom_violin(aes(
    # x = factor(range01(tokyo_kinki_ratio), 3),
    # x = factor(range01(span_tokyo), 3),
    x = factor(range01(span_kinki), 3),
    # y = rt_mean,
    y = is_correct_mean,
    color = phoneme_envs,
    fill = phoneme_envs,
  ))

results_mean %>%
  ggplot() +
  geom_histogram(aes(x = factor(range01(tokyo_kinki_ratio), 4)), stat = "count")
```
```{r}
head(results)
```

全体的に有声音の環境ではアイテムの作成の難しさから弁別が用意になっているかもしれない。

- まぁ普通に近畿で錯覚が増えていることにはならない
- bzの方がps より錯覚が起きやすい
- sp とかがナチュラルかも気になる。多分、気づかないはずだが。

平均を取るのはそこまで問題ではない（正答率がaにあるかbにあるかで変わる）

```{r}
results_mean
```


```{r}
library(lmerTest)
model <- lmer(
  # rt_mean ~
  is_correct_mean ~
    # condition * span_kinki + (1 | subject_id),
    # condition * tokyo_kinki_ratio + (1 | subject_id),
    # condition * tokyo_kinki_ratio + (1 | subject_id) + (1|pair),
    # condition * pair * span_tokyo + (1 | subject_id),
    # condition * pair * span_kinki + (1 | subject_id),
    condition * pair * tokyo_kinki_ratio + (1 | subject_id),
  data = results_mean
  # data = (results_mean %>% filter(pair!="p-s"))
  # data = (results_mean %>% filter(pair %in% c("p-s", "s-p")))
  #data = (results_mean %>% filter(pair %in% c("p-s", "s-p")))
)

# 居住歴の効果はない(トーンの錯覚とは対照的)
summary(model)
```

## バイナリーなのでモデル変更
```{r}
model <- lmer(
  rt ~
    # condition * tokyo_kinki_ratio + (1 | subject_id) + (1|pair),
    # condition * pair * tokyo_kinki_ratio + (1 | subject_id),
    condition * pair * tokyo_kinki_ratio + (1 | subject_id),
  # data = results_filtered
  data = (results_filtered %>% filter(pair!="p-s")),
)
summary(model)
```

```{r}
results_filtered %>% filter(pair %in% c("p-s", "s-p"))
model <- glmer(
  is_correct ~
    # condition * tokyo_kinki_ratio + (1 | subject_id) + (1|pair),
    # condition * pair * tokyo_kinki_ratio + (1 | subject_id),
    condition * pair * tokyo_kinki_ratio + (1 | subject_id),
  # data = results_filtered,
  data = (results_filtered %>% filter(pair %in% c("p-s", "s-p"))),
  family = binomial
)
print(nrow(results_filtered))
# 居住歴の効果はない(トーンの錯覚とは対照的)
summary(model)
```

- voiced になると下がる(ベースライン)
  - p-s, s-p では voiced になると上がる(先行研究と同じ)
  - ts-k は下がる, k-t も下がる（おそらくベースライン）
- p-s や p-s、ts-k は下がる
  - 摩擦や破裂、といった要素でモデリングしたほうが良さそう

効果はなかった。個別の差を分析する感じがよいか。

## デルタ(対応付け)

```{r}
results_mean_delta = results_mean %>% pivot_wider(
  id_cols=c(subject_id, tokyo_kinki_ratio, pair),
  names_from="condition", values_from =is_correct_mean) %>% 
  mutate(delta=voiced-devoiced)
results_mean_delta %>% 
  ggplot() +
  facet_grid(pair ~ .) +
  geom_violin(aes(
    x = factor(range01(tokyo_kinki_ratio), 4),
    y = delta,
    # y = is_correct_mean,
    color = pair,
    fill = pair,
  ))
```
```{r}

model <- lmer(
  delta ~
    tokyo_kinki_ratio + (1 | subject_id),
  data = results_mean_delta
  # data = (results_filtered %>% filter(pair!="p-s")),
)
summary(model)
```

```{r}
print("EOF")
```
